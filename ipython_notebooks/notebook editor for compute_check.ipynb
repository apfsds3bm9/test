{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "associatedRecipe": "compute_check",
    "creator": "thinhprovc202@gmail.com",
    "createdOn": 1728582459403,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {}
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import dataiku\n",
        "from dataiku import spark as dkuspark\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "sc \u003d SparkContext.getOrCreate()\n",
        "sqlContext \u003d SQLContext(sc)\n",
        "\n",
        "# Read recipe inputs\n",
        "norway_new_car_sales_by_make \u003d dataiku.Dataset(\"norway_new_car_sales_by_make\")\n",
        "norway_new_car_sales_by_make_df \u003d dkuspark.get_dataframe(sqlContext, norway_new_car_sales_by_make)\n",
        "\n",
        "# Compute recipe outputs from inputs\n",
        "# TODO: Replace this part by your actual code that computes the output, as a SparkSQL dataframe\n",
        "check_df \u003d norway_new_car_sales_by_make_df # For this sample code, simply copy input to output\n",
        "\n",
        "# Write recipe outputs\n",
        "check \u003d dataiku.Dataset(\"check\")\n",
        "dkuspark.write_with_schema(check, check_df)\n",
        "\n",
        "\n",
        "import dataiku\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Tạo Spark session\n",
        "spark \u003d SparkSession.builder.appName(\"Create Partitioned Datasets\").getOrCreate()\n",
        "\n",
        "# Đọc dataset từ Dataiku\n",
        "df \u003d dataiku.Dataset(\"norway_new_car_sales_by_make\").get_dataframe()\n",
        "\n",
        "# Chuyển đổi DataFrame sang Spark DataFrame\n",
        "spark_df \u003d spark.createDataFrame(df)\n",
        "\n",
        "# Lọc dữ liệu cho năm 2009\n",
        "df_2009 \u003d spark_df.filter(F.col(\u0027Year\u0027) \u003d\u003d 2009)\n",
        "\n",
        "# Lọc dữ liệu cho năm 2010\n",
        "df_2010 \u003d spark_df.filter(F.col(\u0027Year\u0027) \u003d\u003d 2010)\n",
        "\n",
        "# Định nghĩa đường dẫn lưu dataset với partition\n",
        "output_path_2009 \u003d \"norway_new_car_sales_by_make_2009\"\n",
        "output_path_2010 \u003d \"norway_new_car_sales_by_make_2010\"\n",
        "\n",
        "# Lưu dataset cho năm 2009 với partition\n",
        "df_2009.write.partitionBy(\"Year\").format(\"parquet\").mode(\"overwrite\").save(output_path_2009)\n",
        "\n",
        "# Lưu dataset cho năm 2010 với partition\n",
        "df_2010.write.partitionBy(\"Year\").format(\"parquet\").mode(\"overwrite\").save(output_path_2010)"
      ]
    }
  ]
}