{
  "metadata": {
    "analyzedDataset": "heart_measures_train",
    "createdOn": 1682585335302,
    "creator": "clemence.bic@dataiku.com",
    "customFields": {},
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python (env py310_qstest)",
      "language": "python",
      "name": "py-dku-venv-py310_qstest"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "modifiedBy": "yvaidya",
    "tags": [],
    "versionNumber": 1
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Test different Machine Learning models for heart failures prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we will test different Machine Learning approaches to predict heart failures using [scikit-learn](https://scikit-learn.org/stable/)  models (logistic regression, SVM, decision tree, and random forest). For each model, we will first perform a grid search to find the best parameters, then train the model on the train set using these best parameters and finally log everything (parameters, performance metrics, and models) to keep track of the results of our different experiments and be able to compare afterward.\n",
        "Our [Experiment Tracking capability](https://doc.dataiku.com/dss/latest/mlops/experiment-tracking/index.html) relies on the [MLflow framework](https://www.mlflow.org/docs/latest/tracking.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Tip:* Experiment Tracking allows you to save all experiment-related information that you care about for every experiment you run. In Dataiku, this can be done when coding using the [MLflow Tracking API](https://www.mlflow.org/docs/latest/tracking.html). You can then explore and compare all your experiments in the Experiment Tracking UI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Make sure you\u0027re using the correct code environment** (see prerequisites)\n",
        "\n",
        "To be sure, go to **Kernel \u003e Change kernel** and choose `py_quickstart`"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\n",
        "from dataiku import pandasutils as pdu\n",
        "import pandas as pd\n",
        "from utils import model_training\n",
        "import mlflow\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\u0027ignore\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "client \u003d dataiku.api_client()\n",
        "client._session.verify \u003d False"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import the train dataset"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_heart_measures_train \u003d dataiku.Dataset(\"heart_measures_train\")\n",
        "df \u003d dataset_heart_measures_train.get_dataframe(limit\u003d100000)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Set the experiment environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we would like to keep track of all the experiment-related information (performance metrics, parameters and models) for our different ML experiments, we must use a Dataiku managed folder to store all this information. This section is about creating (or accessing if it already exists) the required managed folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Set the required parameters for creating/accessing the managed folder"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set parameters\n",
        "experiment_name \u003d \"Binary Heart Disease Classification\"\n",
        "experiments_managed_folder_name \u003d \"Binary classif experiments\"\n",
        "project \u003d client.get_default_project()\n",
        "mlflow_extension \u003d project.get_mlflow_extension()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Create/access the managed folder"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create the managed folder if it doesn\u0027t exist\n",
        "if experiments_managed_folder_name not in [folder[\u0027name\u0027] for folder in project.list_managed_folders()]:\n",
        "    connections \u003d client.list_connections_names(\u0027all\u0027)\n",
        "    for connection in connections:\n",
        "        try:\n",
        "            project.create_managed_folder(experiments_managed_folder_name, connection_name\u003dconnection)\n",
        "            break\n",
        "        except Exception:\n",
        "            continue\n",
        "    \n",
        "# Get the managed folder id\n",
        "experiments_managed_folder_id \u003d [folder[\u0027id\u0027] for folder in project.list_managed_folders() if folder[\u0027name\u0027]\u003d\u003dexperiments_managed_folder_name][0] \n",
        "\n",
        "# Get the managed folder using the id\n",
        "experiments_managed_folder \u003d project.get_managed_folder(experiments_managed_folder_id)  "
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Prepare data for training"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare data for experiment\n",
        "target\u003d [\"HeartDisease\"]\n",
        "X \u003d df.drop(target, axis\u003d1)\n",
        "y \u003d df[target[0]]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test different modeling approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section will test different models: a Logistic Regression, an SVM, a Decision Tree, and a Random Forest. For each type of model, we will proceed in several steps:\n",
        "\n",
        "1. Set the experiment (where to log the results) and start a new run.\n",
        "\n",
        "2. Define the set of hyperparameters to test.\n",
        "\n",
        "3. Perform a grid search on these hyperparameters using the ``find_best_parameters`` function from the ``model_training.py`` file in the project library.\n",
        "\t\n",
        "\n",
        "4. Cross-evaluate the model with the best hyperparameters on 5 folds using the ```cross_validate_scores``` function from the ```model_training.py``` file in the project library.\n",
        "\n",
        "5. Train the model on the train set using the best hyperparameters.\n",
        "\n",
        "6. Log the experiment\u0027s results (parameters, performance metrics, and model).\n",
        "\n",
        "\n",
        "You can find more information on the tracking APIs in the [MLflow tracking documentation](https://www.mlflow.org/docs/latest/tracking.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the [Scikit-Learn Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with project.setup_mlflow(managed_folder\u003dexperiments_managed_folder) as mlflow:\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    with mlflow.start_run(run_name\u003d\"Linear Regression\"):\n",
        "        \n",
        "        # Find best hyper parameters using a grid search\n",
        "        lr \u003d LogisticRegression(random_state \u003d 42)\n",
        "        cv \u003d 5\n",
        "        params \u003d {\u0027penalty\u0027:[\u0027none\u0027,\u0027l2\u0027]}\n",
        "        scoring \u003d [\u0027accuracy\u0027, \u0027precision\u0027, \u0027recall\u0027, \u0027roc_auc\u0027, \u0027f1\u0027]\n",
        "        print(\"Searching for best parameters...\")\n",
        "        lr_best_params \u003d model_training.find_best_parameters(X, y, lr, params, cv\u003dcv)\n",
        "        print(f\"Best parameters: {lr_best_params}\")\n",
        "        \n",
        "        # Set the best hyper parameters\n",
        "        lr.set_params(**lr_best_params)\n",
        "        \n",
        "        # Cross evaluate the model on the best hyper parameters\n",
        "        lr_metrics_results \u003d model_training.cross_validate_scores(X, y, lr, cv\u003dcv, scoring\u003dscoring)\n",
        "        print(f\u0027Average values for evaluation metrics after cross validation: {\", \".join(f\"{key}: {round(value, 2)}\" for key, value in lr_metrics_results.items())}\u0027)\n",
        "        \n",
        "        # Train the model on the whole train set\n",
        "        lr.fit(X,y)\n",
        "        \n",
        "        # Log the experiment results \n",
        "        mlflow.log_params(lr_best_params)\n",
        "        mlflow.log_metrics(lr_metrics_results)\n",
        "        mlflow.sklearn.log_model(lr, artifact_path\u003d\"model\")\n",
        "        print(\"Best parameters, cross validation metrics, and the model have been saved to Experiment Tracking\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Support Vector Machine:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the [Scikit-Learn SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with project.setup_mlflow(managed_folder\u003dexperiments_managed_folder) as mlflow:\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    with mlflow.start_run(run_name\u003d\"SVM\"):\n",
        "        \n",
        "        # Find best hyper parameters using a grid search\n",
        "        svm \u003d SVC(random_state \u003d 42)\n",
        "        cv \u003d 5\n",
        "        params \u003d {\u0027C\u0027: [0.1,1, 10], \u0027gamma\u0027: [1,0.1,0.01,0.001],\u0027kernel\u0027: [\u0027rbf\u0027, \u0027poly\u0027, \u0027sigmoid\u0027]}\n",
        "        scoring \u003d [\u0027accuracy\u0027, \u0027precision\u0027, \u0027recall\u0027, \u0027roc_auc\u0027, \u0027f1\u0027]\n",
        "        print(\"Searching for best parameters...\")\n",
        "        svm_best_params \u003d model_training.find_best_parameters(X, y, svm, params, cv\u003dcv)\n",
        "        print(f\"Best parameters: {svm_best_params}\")\n",
        "        \n",
        "        # Set the best hyper parameters\n",
        "        svm.set_params(**svm_best_params)\n",
        "        \n",
        "        # Cross evaluate the model on the best hyper parameters\n",
        "        svm_metrics_results \u003d model_training.cross_validate_scores(X, y, svm, cv\u003dcv, scoring\u003dscoring)\n",
        "        print(f\u0027Average values for evaluation metrics after cross validation: {\", \".join(f\"{key}: {round(value, 2)}\" for key, value in svm_metrics_results.items())}\u0027)\n",
        "        \n",
        "        # Train the model on the whole train set\n",
        "        svm.fit(X,y)\n",
        "        \n",
        "        # Log the experiment results \n",
        "        mlflow.log_params(svm_best_params)\n",
        "        mlflow.log_metrics(svm_metrics_results)\n",
        "        mlflow.sklearn.log_model(svm, artifact_path\u003d\"model\")\n",
        "        print(\"Best parameters, cross validation metrics, and the model have been saved to Experiment Tracking\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Decision Tree:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the [Scikit-Learn Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) model."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with project.setup_mlflow(managed_folder\u003dexperiments_managed_folder) as mlflow:\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    with mlflow.start_run(run_name\u003d\"Decision Tree\"):\n",
        "        \n",
        "        # Find best hyper parameters using a grid search\n",
        "        dtc \u003d DecisionTreeClassifier(random_state \u003d 42)\n",
        "        cv \u003d 5\n",
        "        params \u003d {\u0027max_depth\u0027 : [4,5,6,7,8],\n",
        "                  \u0027criterion\u0027 :[\u0027gini\u0027, \u0027entropy\u0027]}\n",
        "        scoring \u003d [\u0027accuracy\u0027, \u0027precision\u0027, \u0027recall\u0027, \u0027roc_auc\u0027, \u0027f1\u0027]\n",
        "        print(\"Searching for best parameters...\")\n",
        "        dtc_best_params \u003d model_training.find_best_parameters(X, y, dtc, params, cv\u003dcv)\n",
        "        print(f\"Best parameters: {dtc_best_params}\")\n",
        "        \n",
        "        # Set the best hyper parameters\n",
        "        dtc.set_params(**dtc_best_params)\n",
        "        \n",
        "        # Cross evaluate the model on the best hyper parameters\n",
        "        dtc_metrics_results \u003d model_training.cross_validate_scores(X, y, dtc, cv\u003dcv, scoring\u003dscoring)\n",
        "        print(f\u0027Average values for evaluation metrics after cross validation: {\", \".join(f\"{key}: {round(value, 2)}\" for key, value in dtc_metrics_results.items())}\u0027)\n",
        "        \n",
        "        # Train the model on the whole train set\n",
        "        dtc.fit(X,y)\n",
        "        \n",
        "        # Log the experiment results \n",
        "        mlflow.log_params(dtc_best_params)\n",
        "        mlflow.log_metrics(dtc_metrics_results)\n",
        "        mlflow.sklearn.log_model(dtc, artifact_path\u003d\"model\")\n",
        "        print(\"Best parameters, cross validation metrics, and the model have been saved to Experiment Tracking\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Random Forest:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the [Scikit-Learn Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "with project.setup_mlflow(managed_folder\u003dexperiments_managed_folder) as mlflow:\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    with mlflow.start_run(run_name\u003d\"Random Forest\"):\n",
        "        \n",
        "        # Find best parameters and cross evaluate the model on the best parameters\n",
        "        rfc \u003d RandomForestClassifier(random_state \u003d 42)\n",
        "        cv \u003d 5\n",
        "        params \u003d {\u0027n_estimators\u0027: [100,200,300],\n",
        "                  \u0027max_depth\u0027 : [5,6,7],\n",
        "                  \u0027criterion\u0027 :[\u0027gini\u0027, \u0027entropy\u0027]}\n",
        "        scoring \u003d [\u0027accuracy\u0027, \u0027precision\u0027, \u0027recall\u0027, \u0027roc_auc\u0027, \u0027f1\u0027]\n",
        "        print(\"Searching for best parameters...\")\n",
        "        rfc_best_params \u003d model_training.find_best_parameters(X, y, rfc, params, cv\u003dcv)\n",
        "        print(f\"Best parameters: {rfc_best_params}\")\n",
        "        \n",
        "        # Set the best hyper parameters\n",
        "        rfc.set_params(**rfc_best_params)\n",
        "\n",
        "        # Cross evaluate the model on the best hyper parameters\n",
        "        rfc_metrics_results \u003d model_training.cross_validate_scores(X, y, rfc, cv\u003dcv, scoring\u003dscoring)\n",
        "        print(f\u0027Average values for evaluation metrics after cross validation: {\", \".join(f\"{key}: {round(value, 2)}\" for key, value in rfc_metrics_results.items())}\u0027)\n",
        "        \n",
        "        # Train the model using the best parameters\n",
        "        rfc.fit(X,y)\n",
        "        \n",
        "        # Log the experiment results \n",
        "        mlflow.log_params(rfc_best_params)\n",
        "        mlflow.log_metrics(rfc_metrics_results)\n",
        "        mlflow.sklearn.log_model(rfc, artifact_path\u003d\"model\")\n",
        "        print(\"Best parameters, cross validation metrics, and the model have been saved to Experiment Tracking\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Explore the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All done! We can now look at the results \u0026 compare our different models by going to theExperiment Tracking page (on the top bar, hover over the circle icon, and select **Experiment Tracking**."
      ]
    }
  ]
}