{
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "tags": [],
    "customFields": {},
    "creator": "clemence.bic",
    "modifiedBy": "yvaidya",
    "createdOn": 1683130614050,
    "versionNumber": 1
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Create a Dataiku Saved Model using the best-performing model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataiku offers pre-built capabilities to evaluate, deploy \u0026 monitor Machine Learning models. Our Python model needs to be stored as a Dataiku Saved Model to benefit these capabilities. \nIn this notebook, we will collect the best model optimizing the accuracy metric from our previous experiment, and deploy it in the Flow as a Dataiku Saved Model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Tip:* Creating a Dataiku Saved Model will allow you to benefit from a set of pre-built evaluation interfaces along with deployment and monitoring capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Make sure you\u0027re using the correct code environment** (see prerequisites)\n\nTo be sure, go to **Kernel \u003e Change kernel** and choose the `py_quickstart`"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\nwarnings.filterwarnings(\u0027ignore\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd\nimport mlflow\nfrom dataikuapi.dss.ml import DSSPredictionMLTaskSettings"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "client \u003d dataiku.api_client()\nclient._session.verify \u003d False"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Get access to the ML experiment information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we use the Dataiku Python API to access to the managed folder where the results of our experiments are stored. "
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set parameters\nexperiment_name \u003d \"Binary Heart Disease Classification\"\nexperiments_managed_folder_name \u003d \"Binary classif experiments\"\n\n# Get various handles\nproject \u003d client.get_default_project()\nmlflow_extension \u003d project.get_mlflow_extension()\nexperiments_managed_folder_id \u003d dataiku.Folder(experiments_managed_folder_name).get_id()\nexperiments_managed_folder \u003d project.get_managed_folder(experiments_managed_folder_id)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Select the experiment with the best accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let\u0027s retrieve the run that generated the best model optimizing the accuracy from our Machine Learning experiments."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimized_metric \u003d \"accuracy\" # You can switch this parameter to another performance metric\n\nwith project.setup_mlflow(managed_folder\u003dexperiments_managed_folder) as mlflow:\n    experiment \u003d mlflow.set_experiment(experiment_name)\n    best_run \u003d mlflow.search_runs(experiment_ids\u003d[experiment.experiment_id], \n                                  order_by\u003d[f\"metrics.{optimized_metric} DESC\"], \n                                  max_results\u003d1, \n                                  output_format\u003d\"list\")[0]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create or get a Dataiku Saved Model using the API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we use the Dataiku Python API to create (or get if it already exists) the Dataiku Saved Model that will be used to deploy our Python model in the Flow."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get or create SavedModel\nsm_name \u003d \"heart-disease-clf\"\nsm_id \u003d None\nfor sm in project.list_saved_models():\n    if sm_name !\u003d sm[\"name\"]:\n        continue\n    else:\n        sm_id \u003d sm[\"id\"]\n        print(\"Found SavedModel {} with id {}\".format(sm_name, sm_id))\n        break\nif sm_id:\n    sm \u003d project.get_saved_model(sm_id)\nelse:\n    sm \u003d project.create_mlflow_pyfunc_model(name\u003dsm_name,\n                                            prediction_type\u003dDSSPredictionMLTaskSettings.PredictionTypes.BINARY)\n    sm_id \u003d sm.id\n    print(\"SavedModel not found, created new one with id {}\".format(sm_id))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Import the new mlflow model into a Saved Model version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let\u0027s import the model from our best run as a new version of the Dataiku Saved Model and make sure it automatically computes performance metrics \u0026 charts based on the train set."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set version ID (a Saved Model can have multiple versions).\n\nif len(sm.list_versions()) \u003d\u003d 0:\n    version_id \u003d \"V1\"\nelse:\n    max_version_num \u003d max([int(v[\u0027id\u0027][1:]) for v in sm.list_versions()])\n    version_id \u003d f\"V{max_version_num+1}\"\n\n# Create version in SavedModel\nsm_version \u003d sm.import_mlflow_version_from_managed_folder(version_id\u003dversion_id,\n                                                         managed_folder\u003dexperiments_managed_folder,\n                                                         path\u003dbest_run.info.artifact_uri.split(experiments_managed_folder_id)[1]+\u0027/model\u0027)\n\n# Evaluate the version using the previously created Dataset\nsm_version.set_core_metadata(target_column_name\u003d\"HeartDisease\",\n                             class_labels\u003d[0, 1],\n                             get_features_from_dataset\u003d\"heart_measures_train\")\n\nsm_version.evaluate(\"heart_measures_train\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Next: use this notebook to create a new step in the pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Create a new step in the flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that our notebook is up and running, we can use it to create the second step of our pipeline in the Flow: \n\n- Click on the **+ Create Recipe** button at the top right of the screen. \n\n- Select the **Python recipe** option.\n\n- Add two **inputs**: the ``heart_measures_train`` dataset and the ``Binary classif experiments`` folder.\n\n- Add the ``heart-disease-clf`` Saved Model as the **output**: **Add** \u003e **Use existing** (option at the bottom).\n\n- Click on the **Create the recipe** button.\n\n- Run the recipe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can explore all the built-in evaluation metrics \u0026 charts of your Python model by clicking on the Saved Model in the Flow. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Evaluate the model on the test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that the model has been deployed on the Flow, we can evaluate it on our test dataset:\n\n- Select the ``heart-disease-clf`` Saved Model.\n\n- On the action panel, select the **Evaluate** recipe.\n\n- On the settings tab, select the ``heart_measures_test`` as the input dataset.\n\n-  For the output, let’s create the ‘Output dataset’ (let’s call it ``heart_measures_test_prediction``), the ‘Metrics’ dataset  (let’s call it ``evaluation_metrics``) and the \u0027Evaluation Store\u0027 (let\u0027s name it ``eval_heart_prediction``)\n\n- Click on the **Create recipe** button.\n\n- Run the recipe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Success! Our model is now deployed on the Flow, it can be used for inference on new datasets and be deployed for production."
      ]
    }
  ]
}